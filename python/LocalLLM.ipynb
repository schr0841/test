{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a66026",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "I have previously seen code to run a local llm in python but i cant recall any of it. can you walk me through the steps and python code to do that? be sure to mention any pitfalls and ways to avoid them on a windows machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c454818",
   "metadata": {},
   "source": [
    "Got you. Here‚Äôs a clean, practical path to running a local LLM on **Windows** with Python, plus the ‚Äúgotchas‚Äù you‚Äôll want to avoid.\n",
    "\n",
    "# 3 good ways to run a local LLM (Windows)\n",
    "\n",
    "## 1) **Ollama** (easiest end-to-end, great for beginners)\n",
    "\n",
    "Ollama runs models locally (CPU/GPU) and exposes a simple HTTP API. There‚Äôs a tiny Python client too.\n",
    "\n",
    "### Install\n",
    "\n",
    "1. Install Ollama for Windows ‚Üí run the installer and launch it. ([Ollama][1])\n",
    "2. Pull a model (examples: `llama3`, `phi4`, `qwen2.5`, `gemma3`):\n",
    "\n",
    "```powershell\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "3. (Optional) Confirm Windows + AMD/NVIDIA GPU acceleration support exists (it does; Windows builds arrived in 2024 and AMD support followed). ([Ollama][2])\n",
    "\n",
    "### Minimal Python (chat once)\n",
    "\n",
    "```python\n",
    "# pip install ollama\n",
    "import ollama\n",
    "resp = ollama.chat(model=\"llama3\", messages=[{\"role\": \"user\", \"content\": \"Explain RBF kernel simply.\"}])\n",
    "print(resp[\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "The official Python client just needs Ollama running and a pulled model. ([GitHub][3])\n",
    "\n",
    "---\n",
    "\n",
    "## 2) **llama.cpp via llama-cpp-python** (loads GGUF files directly in Python)\n",
    "\n",
    "This gives you a pure-Python API (no server needed) for quantized **GGUF** models.\n",
    "\n",
    "### Install (choose your path)\n",
    "\n",
    "* **GPU (NVIDIA CUDA):** install a prebuilt wheel matching your CUDA version, e.g.:\n",
    "\n",
    "```powershell\n",
    "pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "```\n",
    "\n",
    "(Use `cu121`, `cu122`, `cu123`, `cu124`, or `cu125` to match your CUDA.) ([GitHub][4])\n",
    "There are also community prebuilt wheels targeting newer CUDA (e.g., 12.8) if you need them. ([Reddit][5], [Hugging Face][6])\n",
    "\n",
    "* **CPU-only:**\n",
    "\n",
    "```powershell\n",
    "pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "### Minimal Python (load a local GGUF and generate)\n",
    "\n",
    "```python\n",
    "# pip install llama-cpp-python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Point to your downloaded .gguf (e.g., from Hugging Face)\n",
    "llm = Llama(model_path=r\"C:\\models\\Llama-3-8B.Q4_K_M.gguf\", n_ctx=4096, n_gpu_layers=-1)  # set -1 to use GPU if built with CUDA\n",
    "out = llm(\"Write a one-sentence summary of PCA.\\n\", max_tokens=128, stop=[\"\\n\"])\n",
    "print(out[\"choices\"][0][\"text\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) **Transformers** (native PyTorch; flexible, but VRAM-hungry)\n",
    "\n",
    "If you have a decent NVIDIA GPU, this is the most flexible route. If you‚Äôre on **AMD/Intel GPU on Windows**, use **DirectML** for acceleration (works with many‚Äîbut not all‚Äîmodels). ([Microsoft Learn][7], [PyPI][8], [GitHub][9])\n",
    "\n",
    "### Install (NVIDIA CUDA)\n",
    "\n",
    "```powershell\n",
    "# Pick a PyTorch that matches your CUDA (from pytorch.org)\n",
    "pip install torch torchvision torchaudio  # with CUDA wheels\n",
    "pip install transformers accelerate\n",
    "```\n",
    "\n",
    "### Install (AMD/Intel via DirectML)\n",
    "\n",
    "```powershell\n",
    "pip install torch-directml transformers accelerate\n",
    "```\n",
    "\n",
    "Then in Python:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "device = torch.device(\"dml\")  # DirectML device\n",
    "```\n",
    "\n",
    "([Microsoft Learn][7], [PyPI][8])\n",
    "\n",
    "### Minimal Python (Transformers generate)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3-8b-instruct\"   # swap to a license-friendly local mirror/variant as needed\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "# For NVIDIA: device=\"cuda\"; for DirectML: device=\"dml\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"dml\" if torch.backends.__dict__.get(\"mps_is_available\", False) is False else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=device)\n",
    "\n",
    "prompt = \"Explain the bias-variance tradeoff for a stats student.\"\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=160)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "> Want **throughput** or **OpenAI-style server** on your machine? Consider **vLLM**‚Äîit‚Äôs very fast but best on Linux. On Windows, people commonly use **WSL2** (Ubuntu) for vLLM; guides exist that show vLLM running on WSL2. ([DEV Community][10], [Medium][11])\n",
    "\n",
    "---\n",
    "\n",
    "# Windows-specific pitfalls (and how to dodge them)\n",
    "\n",
    "* **GPU not used / falling back to CPU:**\n",
    "\n",
    "  * For **Ollama**, ensure the Windows build + drivers are installed; AMD acceleration is supported (preview rolled out in 2024). Check Task Manager ‚Üí GPU. ([Ollama][2])\n",
    "  * For **Transformers**, your **PyTorch** build must match your driver/CUDA. If you‚Äôre on **AMD/Intel**, use **torch-directml** and set `device=\"dml\"`. ([Microsoft Learn][7], [PyPI][8])\n",
    "\n",
    "* **llama-cpp-python compile errors on Windows:**\n",
    "  Use a **prebuilt wheel** for your CUDA version instead of compiling; compiling requires VS ‚ÄúDesktop development with C++‚Äù and can be finicky. ([GitHub][4])\n",
    "\n",
    "* **bitsandbytes (8-bit) on Windows:**\n",
    "  Upstream **bitsandbytes** does **not** officially support Windows. Workarounds exist but are fragile. If you need quantization on Windows, prefer **GGUF with llama.cpp** or AWQ/GPTQ variants loaded via other backends. ([PyPI][12], [mindfiretechnology.com][13], [GitHub][14])\n",
    "\n",
    "* **Not enough VRAM / OOM:**\n",
    "\n",
    "  * Pick **smaller or more aggressively-quantized** models (e.g., Q4\\_K\\_M GGUF).\n",
    "  * Increase Windows **pagefile** size if you see CPU RAM OOMs (helps stability).\n",
    "  * For Transformers, you can try `torch_dtype=torch.float16`, `max_memory`, or quantized checkpoints.\n",
    "\n",
    "* **Long paths / antivirus interference:**\n",
    "  Keep model paths short (e.g., `C:\\models\\...`) and whitelist your model folder in AV if downloads repeatedly fail/corrupt.\n",
    "\n",
    "* **Licenses & model cards:**\n",
    "  Some checkpoints require acceptance/sign-in (e.g., Meta Llama). Make sure you comply with licensing before scripting downloads.\n",
    "\n",
    "* **WSL2 vs native Windows:**\n",
    "  If you need **vLLM** or Linux-first tooling, WSL2 (Ubuntu) is often the smoothest path on a Windows laptop or desktop. ([DEV Community][10], [Medium][11])\n",
    "\n",
    "---\n",
    "\n",
    "# Quick ‚Äúwhich should I use?‚Äù cheat\n",
    "\n",
    "* **I want the simplest local setup + a Python API.** ‚Üí **Ollama** + `pip install ollama`. ([GitHub][3])\n",
    "* **I want tight Python control + tiny RAM/VRAM usage.** ‚Üí **llama-cpp-python** with **GGUF**. ([GitHub][4])\n",
    "* **I want the full Hugging Face ecosystem.** ‚Üí **Transformers** (CUDA or DirectML). ([Microsoft Learn][7], [PyPI][8])\n",
    "* **I need high-throughput serving.** ‚Üí **vLLM** (prefer **WSL2** on Windows). ([DEV Community][10])\n",
    "\n",
    "If you tell me your GPU/VRAM and whether you prefer server-style vs. pure-Python, I‚Äôll tailor the exact commands and a starter script for your setup.\n",
    "\n",
    "[1]: https://ollama.com/download/windows?utm_source=chatgpt.com \"Download Ollama on Windows\"\n",
    "[2]: https://ollama.com/blog?utm_source=chatgpt.com \"Blog\"\n",
    "[3]: https://github.com/ollama/ollama-python?utm_source=chatgpt.com \"Ollama Python library\"\n",
    "[4]: https://github.com/abetlen/llama-cpp-python?utm_source=chatgpt.com \"Python bindings for llama.cpp\"\n",
    "[5]: https://www.reddit.com/r/LocalLLaMA/comments/1k8xu4d/release_llamacpppython_038_cuda_128_prebuilt/?utm_source=chatgpt.com \"üöÄ [Release] llama-cpp-python 0.3.8 (CUDA 12.8) Prebuilt ...\"\n",
    "[6]: https://huggingface.co/boneylizardwizard/llama-cpp-python-038-cu128-gemma3-wheel?utm_source=chatgpt.com \"boneylizardwizard/llama-cpp-python-038-cu128-gemma3- ...\"\n",
    "[7]: https://learn.microsoft.com/en-us/windows/ai/directml/pytorch-windows?utm_source=chatgpt.com \"Enable PyTorch with DirectML on Windows\"\n",
    "[8]: https://pypi.org/project/torch-directml/?utm_source=chatgpt.com \"torch-directml\"\n",
    "[9]: https://github.com/microsoft/DirectML/blob/master/PyTorch/README.md?utm_source=chatgpt.com \"DirectML/PyTorch/README.md at master\"\n",
    "[10]: https://dev.to/docteurrs/making-vllm-work-on-wsl2-482e?utm_source=chatgpt.com \"Making VLLM work on WSL2\"\n",
    "[11]: https://medium.com/%40techofhp/running-vllm-locally-with-qwen-on-wsl-a-step-by-step-guide-063ad5d1d887?utm_source=chatgpt.com \"Running vLLM locally with Qwen on WSL: A Step-by- ...\"\n",
    "[12]: https://pypi.org/project/bitsandbytes-windows/?utm_source=chatgpt.com \"bitsandbytes-windows\"\n",
    "[13]: https://www.mindfiretechnology.com/blog/archive/installing-bitsandbtyes-for-windows-so-that-you-can-do-peft/?utm_source=chatgpt.com \"Installing BitsAndBtyes for Windows - So That You Can Do ...\"\n",
    "[14]: https://github.com/fa0311/bitsandbytes-windows?utm_source=chatgpt.com \"fa0311/bitsandbytes-windows: 8-bit CUDA functions for ...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2934114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
