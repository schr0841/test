{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae992a15",
   "metadata": {},
   "source": [
    "# Active Inference book\n",
    "\n",
    "## Thomas Parr Giovanni Pezzulo_ Karl J_ Friston -- 2022 -- MIT Press"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84878b6e",
   "metadata": {},
   "source": [
    "##  Mostly coding stuff will appear here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7478dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each chapter?  maybe there are only a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc746085",
   "metadata": {},
   "source": [
    "Of course. Active inference can seem abstract at first, but seeing it in action with code can make the concepts much more concrete. Here are three basic Python program examples that demonstrate how to use a Bayesian framework for active inference.\n",
    "\n",
    "These examples will use the `pymdp` library, a popular and well-documented Python package for active inference. It provides a clear structure for building the components of a generative model, which is at the heart of the Bayesian approach in active inference.\n",
    "\n",
    "First, you'll need to install `numpy` and `pymdp`:\n",
    "\n",
    "```bash\n",
    "pip install numpy pymdp\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### Example 1: The \"T-Maze\" - A Classic Decision Task\n",
    "\n",
    "In this classic experiment, an agent is at the bottom of a 'T' and must decide to go left or right. A cue at the beginning of the trial indicates where a reward is located. The agent wants to find the reward and has a preference for being at the reward location.\n",
    "\n",
    "**The Bayesian Framework in Action:**\n",
    "\n",
    "  * **Generative Model:** The agent has an internal model of how the world works. This model includes:\n",
    "      * **Priors over states (D):** The agent's initial belief about where it is.\n",
    "      * **Likelihood model (A):** The probability of receiving a particular observation given the true state of the world.\n",
    "      * **Transition model (B):** The probability of transitioning to a new state given the current state and a chosen action.\n",
    "      * **Priors over policies (E):** The agent's initial preferences for certain sequences of actions.\n",
    "      * **Prior preferences (C):** The agent's goals, expressed as a probability distribution over desired outcomes.\n",
    "  * **Inference:** The agent uses Bayesian inference (specifically, belief propagation or variational inference under the hood in `pymdp`) to update its beliefs about its current state based on observations.\n",
    "  * **Action Selection:** The agent chooses the action that it predicts will minimize its \"expected free energy\" - a quantity that balances exploring to reduce uncertainty (epistemic value) and exploiting to achieve preferred outcomes (pragmatic value).\n",
    "\n",
    "**Python Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7b2509",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A matrix must be a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m C[\u001b[32m1\u001b[39m] = \u001b[32m1.0\u001b[39m \u001b[38;5;66;03m# High preference for being at the 'Left' location\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 2. Create the Agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m my_agent = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m=\u001b[49m\u001b[43mA_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[43mB_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 3. Run the simulation for one trial\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Initial observation: The agent sees the cue pointing 'Left'\u001b[39;00m\n\u001b[32m     43\u001b[39m observation = [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m] \u001b[38;5;66;03m# Sees 'Up' location, sees 'Left' cue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\schre\\anaconda3\\envs\\active_inf_env\\Lib\\site-packages\\pymdp\\agent.py:88\u001b[39m, in \u001b[36mAgent.__init__\u001b[39m\u001b[34m(self, A, B, C, D, E, pA, pB, pD, num_controls, policy_len, inference_horizon, control_fac_idx, policies, gamma, alpha, use_utility, use_states_info_gain, use_param_info_gain, action_selection, sampling_mode, inference_algo, inference_params, modalities_to_learn, lr_pA, factors_to_learn, lr_pB, lr_pD, use_BMA, policy_sep_prior, save_belief_hist)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Initialise observation model (A matrices)\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     89\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA matrix must be a numpy array\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.A = utils.to_obj_array(A)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m utils.is_normalized(\u001b[38;5;28mself\u001b[39m.A), \u001b[33m\"\u001b[39m\u001b[33mA matrix is not normalized (i.e. A.sum(axis = 0) must all equal 1.0)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: A matrix must be a numpy array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymdp import agent\n",
    "\n",
    "# 1. Define the Generative Model\n",
    "\n",
    "# States: [Location (Up, Left, Right), Cue (Left, Right)]\n",
    "# Observations: [Location Observation (See Up, See Left, See Right), Cue Observation (See Left, See Right)]\n",
    "# Controls: [Move (Up, Left, Right)]\n",
    "\n",
    "# Likelihood mapping (A matrix)\n",
    "A = np.zeros((3, 3, 2))\n",
    "A[0, 0, :] = 1.0  # If at 'Up', you observe 'Up'\n",
    "A[1, 1, :] = 1.0  # If at 'Left', you observe 'Left'\n",
    "A[2, 2, :] = 1.0  # If at 'Right', you observe 'Right'\n",
    "\n",
    "A_cue = np.zeros((2, 3, 2))\n",
    "A_cue[0, :, 0] = 1.0 # If cue is 'Left', you observe 'Left'\n",
    "A_cue[1, :, 1] = 1.0 # If cue is 'Right', you observe 'Right'\n",
    "\n",
    "A_list = [A, A_cue]\n",
    "\n",
    "# Transition model (B matrix)\n",
    "B = np.zeros((3, 3, 3))\n",
    "B[0, 0, 0] = 1.0 # From 'Up', taking 'Up' action, stay 'Up'\n",
    "B[1, 0, 1] = 1.0 # From 'Up', taking 'Left' action, go to 'Left'\n",
    "B[2, 0, 2] = 1.0 # From 'Up', taking 'Right' action, go to 'Right'\n",
    "B[:, 1, :] = np.expand_dims(np.eye(3)[:, 1], axis=1) # Stay at Left\n",
    "B[:, 2, :] = np.expand_dims(np.eye(3)[:, 2], axis=1) # Stay at Right\n",
    "\n",
    "B_cue = np.tile(np.eye(2), (3, 1, 1)).T\n",
    "B_list = [B, B_cue]\n",
    "\n",
    "# Prior preferences (C vector) - The agent wants to be at the rewarded location\n",
    "# Let's say the agent wants the 'Left' reward\n",
    "C = np.zeros(3)\n",
    "C[1] = 1.0 # High preference for being at the 'Left' location\n",
    "\n",
    "# 2. Create the Agent\n",
    "my_agent = agent.Agent(A=A_list, B=B_list, C=C)\n",
    "\n",
    "# 3. Run the simulation for one trial\n",
    "# Initial observation: The agent sees the cue pointing 'Left'\n",
    "observation = [0, 0] # Sees 'Up' location, sees 'Left' cue\n",
    "\n",
    "# The agent infers its current state based on the observation\n",
    "qs = my_agent.infer_states(observation)\n",
    "print(f\"Belief about current location after seeing cue: {qs[0]}\")\n",
    "print(f\"Belief about the cue's direction: {qs[1]}\")\n",
    "\n",
    "# The agent infers the best policy (sequence of actions)\n",
    "q_pi, _ = my_agent.infer_policies()\n",
    "print(f\"Beliefs over policies: {q_pi}\")\n",
    "\n",
    "# The agent selects an action\n",
    "action = my_agent.sample_action()\n",
    "print(f\"Action taken: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123486be",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Example 2: Grid World Navigation\n",
    "\n",
    "Here, an agent exists in a simple 2x2 grid world and wants to navigate to a specific goal location.\n",
    "\n",
    "**The Bayesian Framework in Action:**\n",
    "\n",
    "This example further highlights how the agent's beliefs about its own position are updated probabilistically. If the agent isn't certain where it is, it might take an action to confirm its location before heading to the goal.\n",
    "\n",
    "**Python Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2abe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymdp import agent, utils\n",
    "\n",
    "# 1. Define the Generative Model\n",
    "# States: [Location (0, 1, 2, 3)]\n",
    "# Observations: [Location Observation (0, 1, 2, 3)]\n",
    "# Controls: [Move (Stay, Up, Down, Left, Right)]\n",
    "\n",
    "grid_size = 2\n",
    "num_states = grid_size * grid_size\n",
    "num_obs = num_states\n",
    "num_controls = 5\n",
    "\n",
    "# Likelihood mapping (A matrix) - Perfect observation of location\n",
    "A = np.eye(num_states)\n",
    "\n",
    "# Transition model (B matrix)\n",
    "B = np.zeros((num_states, num_states, num_controls))\n",
    "\n",
    "for s in range(num_states):\n",
    "    B[s, s, 0] = 1.0 # Stay\n",
    "    # Implement Up, Down, Left, Right transitions (with boundaries)\n",
    "    row, col = np.unravel_index(s, (grid_size, grid_size))\n",
    "    # Up\n",
    "    next_row = max(0, row - 1)\n",
    "    B[np.ravel_multi_index((next_row, col), (grid_size, grid_size)), s, 1] = 1.0\n",
    "    # Down\n",
    "    next_row = min(grid_size - 1, row + 1)\n",
    "    B[np.ravel_multi_index((next_row, col), (grid_size, grid_size)), s, 2] = 1.0\n",
    "    # Left\n",
    "    next_col = max(0, col - 1)\n",
    "    B[np.ravel_multi_index((row, next_col), (grid_size, grid_size)), s, 3] = 1.0\n",
    "    # Right\n",
    "    next_col = min(grid_size - 1, col + 1)\n",
    "    B[np.ravel_multi_index((row, next_col), (grid_size, grid_size)), s, 4] = 1.0\n",
    "\n",
    "\n",
    "# Prior preferences (C vector) - Goal is location 3 (bottom-right)\n",
    "C = np.zeros(num_obs)\n",
    "C[3] = 1.0\n",
    "\n",
    "# 2. Create the Agent\n",
    "my_agent = agent.Agent(A=A, B=B, C=C)\n",
    "\n",
    "# 3. Run a step of the simulation\n",
    "# Agent's initial belief is that it's at location 0 (top-left)\n",
    "my_agent.qs = utils.onehot(0, num_states)\n",
    "print(f\"Initial belief about location: {my_agent.qs}\")\n",
    "\n",
    "# The agent gets an observation confirming it's at location 0\n",
    "observation = 0\n",
    "\n",
    "# Agent updates its beliefs (though they are already certain here)\n",
    "qs = my_agent.infer_states(observation)\n",
    "\n",
    "# Agent decides on the best policy\n",
    "q_pi, _ = my_agent.infer_policies()\n",
    "print(f\"Beliefs over policies: {q_pi}\") # Should favor moving right or down\n",
    "\n",
    "# Agent takes an action\n",
    "action = my_agent.sample_action()\n",
    "print(f\"Action taken (0=Stay, 1=Up, 2=Down, 3=Left, 4=Right): {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34037437",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### Example 3: Perceptual Categorization - Is it a \"Cat\" or a \"Dog\"?\n",
    "\n",
    "This example focuses on the perceptual aspect of active inference. The agent receives ambiguous sensory data and must infer the hidden cause of that data.\n",
    "\n",
    "**The Bayesian Framework in Action:**\n",
    "\n",
    "This scenario strips away the action component to purely showcase Bayesian belief updating. The agent has prior beliefs about whether it's more likely to see a cat or a dog. It then receives some sensory evidence (e.g., a feature like \"pointy ears\") and updates its beliefs to a posterior distribution.\n",
    "\n",
    "**Python Code:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552abd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymdp import utils\n",
    "\n",
    "# 1. Define the Generative Model\n",
    "\n",
    "# Hidden States: [Identity (Cat, Dog)]\n",
    "# Observations: [Features (Pointy Ears, Floppy Ears)]\n",
    "\n",
    "num_states = 2  # Cat, Dog\n",
    "num_obs = 2     # Pointy, Floppy\n",
    "\n",
    "# Likelihood mapping (A matrix)\n",
    "# P(Observation | State)\n",
    "A = np.zeros((num_obs, num_states))\n",
    "\n",
    "# If it's a Cat, it's likely to have pointy ears\n",
    "A[0, 0] = 0.8  # P(Pointy Ears | Cat)\n",
    "A[1, 0] = 0.2  # P(Floppy Ears | Cat)\n",
    "\n",
    "# If it's a Dog, it's more likely to have floppy ears\n",
    "A[0, 1] = 0.3  # P(Pointy Ears | Dog)\n",
    "A[1, 1] = 0.7  # P(Floppy Ears | Dog)\n",
    "\n",
    "# 2. Define Prior Beliefs\n",
    "\n",
    "# Prior beliefs about the hidden state (D vector)\n",
    "# Let's say the agent thinks it's slightly more likely to see a dog\n",
    "D = np.array([0.4, 0.6]) # P(Cat), P(Dog)\n",
    "print(f\"Prior belief (P(Cat), P(Dog)): {D}\")\n",
    "\n",
    "# 3. Perform Inference after an Observation\n",
    "\n",
    "# The agent observes 'Pointy Ears' (observation index 0)\n",
    "observation_index = 0\n",
    "\n",
    "# Calculate the posterior belief using Bayes' rule (which is what infer_states does)\n",
    "# Posterior = normalize(Likelihood * Prior)\n",
    "likelihood_of_obs = A[observation_index, :]\n",
    "posterior = utils.norm_dist(likelihood_of_obs * D)\n",
    "\n",
    "print(f\"Observation: Pointy Ears\")\n",
    "print(f\"Likelihood of this observation given states: {likelihood_of_obs}\")\n",
    "print(f\"Posterior belief (P(Cat), P(Dog)): {posterior}\")\n",
    "\n",
    "# Now, let's see what happens if it observes 'Floppy Ears'\n",
    "observation_index = 1\n",
    "likelihood_of_obs = A[observation_index, :]\n",
    "posterior = utils.norm_dist(likelihood_of_obs * D)\n",
    "\n",
    "print(f\"\\nObservation: Floppy Ears\")\n",
    "print(f\"Likelihood of this observation given states: {likelihood_of_obs}\")\n",
    "print(f\"Posterior belief (P(Cat), P(Dog)): {posterior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ae17d",
   "metadata": {},
   "source": [
    "\n",
    "These examples provide a starting point for understanding how to implement active inference in Python. By manipulating the `A`, `B`, and `C` matrices, you can build increasingly complex and interesting agent behaviors, all grounded in the core principles of Bayesian inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active_inf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
